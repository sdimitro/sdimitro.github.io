<!DOCTYPE html>
<html lang="en-us">
	<meta charset="UTF-8">
	<title>MDB&#39;s Extensibility - Debugging N-way Deadlocks | Core Dump</title>
	<link href="https://fonts.googleapis.com/css?family=Libre+Baskerville" rel="stylesheet"> 
	<link href="https://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet"> 
	<link rel="stylesheet" href="https://sdimitro.github.io/css/style.css">

<div class="header">
	<h3><a href="https://sdimitro.github.io">Core Dump</a></h3>
</div>

<div class="main">
	<div class="article">
		<h1>MDB&#39;s Extensibility - Debugging N-way Deadlocks
			<div class="normal">
				<div class="when"> Posted on November 3, 2018.</div>
			</div>
		</h1>
   <div class="content">
     

<h3 id="introduction">Introduction</h3>

<p>I was recently looking at an escalation at work with my colleagues Sara Hartse
and John Gallagher. The customer&rsquo;s system had panicked and rebooted twice over
a period of two weeks. Examining the most recent crash dump we found out that
the system had panicked because the OS itself had detected a deadlock. After
poking around, we ended up finding that there was a cycle of 4 ZFS threads
blocked waiting for each other on various mutexes. After going over the locks
involved, Matt Ahrens pointed out the issue and soon after that Sara came up
with a fix.</p>

<p>After this interesting escalation, I became interested in the mechanism of
deadlock detection by the OS. Besides sheer curiosity about something that I
never knew that OS was doing in the runtime, I was also motivated by the fact
that John, Sara, and I had spent quite some time trying to figure out the
threads involved. The system already had that piece of information somewhere,
otherwise it wouldn&rsquo;t have panicked. After reading through the code path
where the panic had happened and going over some more generic mutex code,
I finally figured out how the detection mechanism worked.</p>

<p>In order to test my knowledge, I decided to enhance MDB, our debugger, with
a new command that prints the cycle of threads involved in the mutex deadlock
given the address of any of the mutexes involved. So I implemented that and
ensured that it works on the customers crash dump. Interestingly, while doing
that, I came across some pre-existing basic commands that would do things
like print the owner of a mutex given the mutex&rsquo;s address. So it is not like
no one has done something similar before. Still, there wasn&rsquo;t any command that
given a thread it would print out the lock that the thread is blocked waiting.</p>

<p>I decided to keep my command around. Even though half of its functionality
already exists and now I know how to figure out the rest of the information
by printing different fields from thread structures, I believe there is still
value in it. There may already be a fair amount of pressure involved during an
escalation by all parties involved, that can lead to small mistakes and
repeated work. Why not let the computer do the work for you?</p>

<p>I wrote this post mainly for two reasons. The first is to show how we debug
simple deadlocks currently in illumos. The other is to introduce MDB and
how one may go about enhancing it with new commands.</p>

<h3 id="analyzing-the-deadlock-with-mdb">Analyzing the Deadlock with MDB</h3>

<p>Each crash dump when unpacked/decompressed consists of two parts: The memory
contents at the time of the crash (file name is usually <code>vmcore.&lt;some number&gt;</code>)
and the ELF binary of the kernel (usually <code>unix.&lt;some number&gt;</code>). Both files
are generally unpacked in <code>/var/crash</code> and they are suffixed by the same
number. This way multiple crash dump file pairs are distinguished from each
other under the same directory. The suffixed number can be passed as an
argument to mdb when opening a crash dump.</p>

<pre><code>$ ls
unix.1     vmcore.1
$ mdb 1
Loading modules: [ unix genunix specfs dtrace mac cpu.generic uppc pcplusmp
scsi_vhci zfs sd mpt ip hook neti sockfs arp usba fctl stmf stmf_sbd mm
random lofs idm cpc crypto fcip fcp ufs logindmux nsmb ptm smbsrv klmmod nfs
ipc ]
&gt;
</code></pre>

<p>The generic syntax of mdb commands is the following:</p>

<pre><code>[&lt;optional address&gt;]::&lt;command&gt; [&lt;arguments&gt;]
</code></pre>

<p>In most cases, the first thing you want to check when troubleshooting a panic
is the kernel message ring buffer. This is similar to running <code>dmesg</code> on a
running system. Every time there is a panic, a panic message is written to
the ring buffer. If crash dump collection is enabled, that panic message should
be the last thing on the buffer.</p>

<pre><code>&gt; ::msgbuf
MESSAGE
pcplusmp: ide (ata) instance 0 irq 0xe vector 0x48 ioapic 0x8 intin 0xe is bound to cpu 7
... &lt; more messages &gt; ...
pcplusmp: ide (ata) instance 0 irq 0xe vector 0x48 ioapic 0x8 intin 0xe is bound to cpu 1

panic[cpu2]/thread=ffffff01723c6c40:
Deadlock: cycle in blocking chain


ffffff01723c63e0 genunix:process_type+1e7952 ()
ffffff01723c6450 unix:mutex_vector_enter+3a3 ()
ffffff01723c64c0 zfs:dbuf_find+ac ()
ffffff01723c6550 zfs:dbuf_hold_impl+6a ()
ffffff01723c65a0 zfs:dbuf_hold_level+31 ()
ffffff01723c65d0 zfs:dbuf_hold+21 ()
ffffff01723c6680 zfs:dmu_buf_hold_array_by_dnode+fd ()
ffffff01723c6720 zfs:dmu_write_uio_dnode+5f ()
ffffff01723c67b0 zfs:zvol_write+162 ()
ffffff01723c67e0 genunix:cdev_write+2d ()
ffffff01723c68c0 specfs:spec_write+4c1 ()
ffffff01723c6940 genunix:fop_write+5b ()
ffffff01723c6a20 genunix:vn_rdwr+27a ()
ffffff01723c6ad0 stmf_sbd:sbd_data_write+11c ()
ffffff01723c6b60 stmf_sbd:sbd_handle_write_xfer_completion+1b4 ()
ffffff01723c6b80 stmf_sbd:sbd_dbuf_xfer_done+110 ()
ffffff01723c6c20 stmf:stmf_worker_task+386 ()
ffffff01723c6c30 unix:thread_start+8 ()

dumping to /dev/zvol/dsk/rpool/dump, offset 65536, content: kernel
&gt;
</code></pre>

<p>In the case above we are informed that the system has detected a deadlock.
We are also given the stacktrace of the thread which tried to grab the lock
from which we detected the deadlock. In this case we know that it was some lock
that we tried to acquire from ZFS in <code>dbuf_find</code>.</p>

<p>In order to get the exact address of the mutex we need to print the stacktrace
of the current thread but this time together with the arguments on each frame.
There is a built-in MDB command for this, <code>$C</code>.</p>

<pre><code>&gt; $C
ffffff01723c6340 vpanic()
ffffff01723c63e0 0xfffffffffbe8302a()
ffffff01723c6450 mutex_vector_enter+0x3a3(ffffff4e3e042940)
ffffff01723c64c0 dbuf_find+0xac(ffffff4b200f80c0, 1, 0, 1ba80df)
ffffff01723c6550 dbuf_hold_impl+0x6a(ffffff532d6fab20, 0, 1ba80df, 0, 0,
	fffffffff792b490, ffffff01723c6578)
ffffff01723c65a0 dbuf_hold_level+0x31(ffffff532d6fab20, 0, 1ba80df, fffffffff792b490)
ffffff01723c65d0 dbuf_hold+0x21(ffffff532d6fab20, 1ba80df, fffffffff792b490)
ffffff01723c6680 dmu_buf_hold_array_by_dnode+0xfd(ffffff532d6fab20, 37501b8000, 7000,
	0, fffffffff792b490, ffffff01723c66cc, ffffff01723c66c0, ffffff5300000000)
ffffff01723c6720 dmu_write_uio_dnode+0x5f(ffffff532d6fab20, ffffff01723c6970, 7000,
	ffffff4c89199380)
ffffff01723c67b0 zvol_write+0x162(11d000002bd, ffffff01723c6970, ffffff4a56df9db0)
ffffff01723c67e0 cdev_write+0x2d(11d000002bd, ffffff01723c6970, ffffff4a56df9db0)
ffffff01723c68c0 spec_write+0x4c1(ffffff4b21a3ce00, ffffff01723c6970, 0, ffffff4a56df9db0, 0)
ffffff01723c6940 fop_write+0x5b(ffffff4b21a3ce00, ffffff01723c6970, 0, ffffff4a56df9db0, 0)
ffffff01723c6a20 vn_rdwr+0x27a(1, ffffff4b21a3ce00, ffffff4ba475d000, 7000, 37501b8000, 1,
	ffffffff00000000, fffffffffffffffd, ffffff4a56df9db0, ffffff01723c6a68)
ffffff01723c6ad0 sbd_data_write+0x11c(ffffff4b19acb798, ffffff54a8255000, 37501b8000,
	7000, ffffff4ba475d000)
ffffff01723c6b60 sbd_handle_write_xfer_completion+0x1b4(ffffff54a8255000, ffffff4c90b13c50,
	ffffff54886add00, 1)
ffffff01723c6b80 sbd_dbuf_xfer_done+0x110(ffffff54a8255000, ffffff54886add00)
ffffff01723c6c20 stmf_worker_task+0x386(ffffff4b598be370)
ffffff01723c6c30 thread_start+8()
&gt;
</code></pre>

<p>With that we know that the address of the mutex is <code>ffffff4e3e042940</code>. The
general instinct of a debugger at this point would be to roll the dice and use
something like <code>ffffff4e3e042940::whatthread</code>. The <code>whatthread</code> command takes
the address on the left of the <code>::</code> separator and prints out all the threads
with stacks containing that address. This can be very helpful on some cases.
In our case though, the mutex can be part of a common structure multiple
threads are referencing, regardless of whether they are doing anything related
to the mutex or not. This means that we can have false positives.</p>

<p>Assuming we don&rsquo;t know anything about the implementation of mutexes in illumos
the next step would be to look at the available commands and look for anything
related to mutexes.</p>

<pre><code>&gt; ::dcmds ! grep mutex
mutex                    - dump out an adaptive or spin mutex
&gt;
</code></pre>

<p>In the above example we used <code>::dcmd</code> which prints all the MDB commands
currently available in our session. The bang <code>!</code> after that means that we want
to take the output of that command and pipe its output as an input to a shell
command. We effectively invoke the shell to grep through the output of
<code>::dcmds</code> looking for any occurence of the word mutex, and it seems we are in
luck.</p>

<p>Figuring out how the command works, we use it on our mutex:</p>

<pre><code>&gt; ::help mutex

NAME
  mutex - dump out an adaptive or spin mutex

SYNOPSIS
  addr ::mutex [-f]

DESCRIPTION
  Options:
     -f    force printing even if the data seems to be inconsistent

ATTRIBUTES

  Target: kvm
  Module: genunix
  Interface Stability: Unstable
&gt; ffffff4e3e042940::mutex
            ADDR  TYPE             HELD MINSPL OLDSPL WAITERS
ffffff4e3e042940 adapt ffffff01727edc40      -      -     yes
</code></pre>

<p>So the mutex is held by the thread at address <code>0xffffff01727edc40</code>. Printing
that thread&rsquo;s stack should show us where that thread is stuck.</p>

<pre><code>&gt; ffffff01727edc40::findstack -v
stack pointer for thread ffffff01727edc40: ffffff01727ed330
[ ffffff01727ed330 _resume_from_idle+0x126() ]
  ffffff01727ed360 swtch+0x141()
  ffffff01727ed400 turnstile_block+0x21a(ffffff4ee30a9258, 0, ffffff4a995415d8,
	fffffffffbc20380, 0, 0)
  ffffff01727ed470 mutex_vector_enter+0x3a3(ffffff4a995415d8)
  ffffff01727ed4d0 dsl_pool_dirty_space+0x3e(ffffff4a99541480, 2000,
	ffffff5071a857c0)
  ffffff01727ed520 dmu_objset_willuse_space+0x5c(ffffff4b200f80c0, 2000,
	ffffff5071a857c0)
  ffffff01727ed5a0 dbuf_dirty+0x194(ffffff4e3e0428d8, ffffff5071a857c0)
  ffffff01727ed5e0 dmu_buf_will_dirty+0xdc(ffffff4e3e0428d8, ffffff5071a857c0)
  ffffff01727ed680 dmu_write_uio_dnode+0x90(ffffff532d6fab20, ffffff01727ed8d0,
	8000, ffffff5071a857c0)
  ffffff01727ed710 zvol_write+0x162(11d000002bd, ffffff01727ed8d0, ffffff4a56df9db0)
  ffffff01727ed740 cdev_write+0x2d(11d000002bd, ffffff01727ed8d0, ffffff4a56df9db0)
  ffffff01727ed820 spec_write+0x4c1(ffffff4b21a3ce00, ffffff01727ed8d0, 0,
	ffffff4a56df9db0, 0)
  ffffff01727ed8a0 fop_write+0x5b(ffffff4b21a3ce00, ffffff01727ed8d0, 0,
	ffffff4a56df9db0, 0)
  ffffff01727ed980 vn_rdwr+0x27a(1, ffffff4b21a3ce00, ffffff4d2abb2000, 8000,
	37501bf000, 1, 0, fffffffffffffffd, ffffff4a56df9db0, ffffff01727ed9c8)
  ffffff01727eda30 sbd_data_write+0x11c(ffffff4b19acb798, ffffff52a0583000,
	37501bf000, 8000, ffffff4d2abb2000)
  ffffff01727edac0 sbd_handle_write_xfer_completion+0x1b4(ffffff52a0583000,
	ffffff558cf4ce00, ffffff4b2a6fa4c8, 0)
  ffffff01727edb20 sbd_handle_write+0x3b2(ffffff52a0583000, ffffff4b2a6fa4c8)
  ffffff01727edb80 sbd_new_task+0x2a1(ffffff52a0583000, ffffff4b2a6fa4c8)
  ffffff01727edc20 stmf_worker_task+0x2fd(ffffff4b598bfcc0)
  ffffff01727edc30 thread_start+8()
&gt;
</code></pre>

<p>As expexted, this thread is also stuck waiting on a mutex. Repeating the
commands <code>::mutex</code> and <code>::findstack -v</code> eventually point back to the thread
that we started. At that point, we have a list of all the mutex addresses and
threads involved. The next step would be looking at the stacks together with
the code and figure out the locks involved in the code. That is generally
straightforward except in some cases where there may be multiple calls of
<code>mutex_enter()</code> within the same function for different locks, or the lock
itself is part of a table and we grab it through an index. In those cases
we can figure out which lock, by looking at the assembly close to the
program counter of that frame and/or examining the parameters passed to the
function as arguments in the crash dump.</p>

<hr />

<p>I won&rsquo;t go into the details of the root cause for the deadlock and how we fixed
it. For the ZFS-curious, the bug was that the <code>txg_sync_thread()</code> of the
particular pool was calling into the DMU while holding <code>tx_sync_lock</code> which is
intended to be a leaf lock (i.e. we don&rsquo;t grab other locks while holding it).</p>

<hr />

<h3 id="turnstiles-and-mutexes-in-illumos">Turnstiles and Mutexes in illumos</h3>

<p>A turnstile is effectively a sleeping queue used for managing threads blocking
on a lock. Each kernel thread is born with an attached turnstile. When a
thread needs to block on a synchronization object (e.g. a mutex)</p>

<p>what did you read, what did you learn, pictures, slowly going towards
your goal, mutexes, runistiles, sobjs.</p>

<h3 id="extending-mdb">Extending MDB</h3>

<p>Thought it would be fun attempting to do a dcmd that saves some of the
effort. Half of the work is done <code>::mutex</code></p>

<p>decided to make a walker, what is a walker? When is it a good idea to
implement a walker? When is it not? Why better in this case?</p>

<p>Go over first version with mdb_vread. This is fine but could we do
better?</p>

<p>Second version using mdb_ctf_vread(). Credit Matt.</p>

<p>Why is this better? field from mdb struct field matched with the one
from the kernel.</p>

<p>CTF information, what is it? how is it used? what does it look like?
How does it enable reflection? What does reflection have to do with it?
Field match happens through ctf info in the kernel and in the mdb mod.</p>

<p>using the walker, then combine it with findstacks -v</p>

<h3 id="further-hacking">Further Hacking</h3>

<ul>
<li>print synchonization objects involved besides threads. deals with the
issue of the thread having grabbed multiple locks and you don&rsquo;t know
which one it is from the stack trace because there are mutliple frames
that could have grabbed it.</li>
<li>rwlocks intro, could be extended to include places where a writer holds
the lock and other threads are blocking.</li>
<li>There could be a potential generic solution for rwlocks, CVs and
semaphores, maybe search all the threads available..</li>
</ul>

<h3 id="further-reading">Further Reading</h3>

<p>Synchronization:
- Books (Solaris Internals, Linux Kernel Dev?, I know that lkernel
has its own thing for detection)
Information on CTF (important used by DTrace), Linux has BTF info know
for bpf
MDB books -&gt; illumos book (unfortunately no mdb_ctf_vread) + dev guide
open online -&gt; hardcover book Panic! for history. Other angelic things
blogpost, bug hunter&rsquo;s diary mention?, eschroch ::whatthread
ACID -&gt; extensibility (relied on DWARF, Russ Cox pointing towards CTF).</p>

   </div>
	</div>
</div>
<div id="disqus_thread"></div>
<script>


   var disqus_config = function () {
	   this.page.url = "https://sdimitro.github.io""MDB's Extensibility - Debugging N-way Deadlocks";
	   this.page.identifier = "MDB's Extensibility - Debugging N-way Deadlocks";
   };
(function() { 
	if (window.location.hostname == "localhost")
		return;
	var d = document, s = d.createElement('script');
	s.src = '//sdimitro-github-com.disqus.com/embed.js';
	s.setAttribute('data-timestamp', +new Date());
	(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-78963448-1', 'auto');
ga('send', 'pageview');
</script>


